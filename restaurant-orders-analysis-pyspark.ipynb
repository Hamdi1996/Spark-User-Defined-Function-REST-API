{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2414e762-d4ec-4616-b54c-4a2fe31b2a3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/12/30 00:20:52 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import zipfile\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02441624-3341-45bf-80e3-bb46206557ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName(\"RestaurantOrdersPipeline\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6ed268dc-893e-419e-8b56-54ae7d1ca6b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_zip(zip_file_path, extract_to_dir):\n",
    "    \"\"\"\n",
    "    Extract ZIP file to a specified directory.\n",
    "    \"\"\"\n",
    "    with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
    "        zip_ref.extractall(extract_to_dir)\n",
    "    print(f\"Extracted {zip_file_path} to {extract_to_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3f53a01d-7783-43fc-8f04-23c2e8e86117",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_restaurant_orders_with_menu(file_path_orders, file_path_menu):\n",
    "    \"\"\"\n",
    "    Process the Restaurant Orders dataset with PySpark and join it with menu items.\n",
    "    \"\"\"\n",
    "    orders_df = spark.read.csv(file_path_orders, header=True, inferSchema=True)\n",
    "    menu_df = spark.read.csv(file_path_menu, header=True, inferSchema=True)\n",
    "\n",
    "    print(\"Orders DataFrame Schema:\")\n",
    "    orders_df.printSchema()\n",
    "\n",
    "    print(\"Menu Items DataFrame Schema:\")\n",
    "    menu_df.printSchema()\n",
    "    menu_df = menu_df.withColumnRenamed(\"menu_item_id\", \"item_id\")\n",
    "\n",
    "    # Join the datasets on the item_id column\n",
    "    joined_df = orders_df.join(menu_df, on=\"item_id\", how=\"inner\")\n",
    "    print(\"Joined DataFrame:\")\n",
    "    joined_df.show(truncate=False)\n",
    "\n",
    "    #Total revenue per date\n",
    "    revenue_per_date = joined_df.groupBy(\"order_date\").agg((joined_df[\"price\"]).alias(\"TotalRevenue\")).orderBy(\"order_date\")\n",
    "    print(\"Total Revenue Per Date:\")\n",
    "    revenue_per_date.show()\n",
    "\n",
    "    #Top-selling items\n",
    "    top_items = joined_df.groupBy(\"item_name\").count().orderBy(\"count\", ascending=False)\n",
    "    print(\"Top-Selling Items:\")\n",
    "    top_items.show()\n",
    "\n",
    "    #Revenue by item category\n",
    "    revenue_by_category = joined_df.groupBy(\"category\").agg((joined_df[\"price\"]).alias(\"TotalRevenue\")\n",
    "                                                           ).orderBy(\"TotalRevenue\", ascending=False)\n",
    "\n",
    "    print(\"Revenue By Item Category:\")\n",
    "    revenue_by_category.show()\n",
    "\n",
    "    # Save the analysis results\n",
    "    revenue_per_date.write.csv(\"output/revenue_per_date.csv\", mode=\"overwrite\", header=True)\n",
    "    top_items.write.csv(\"output/top_items.csv\", mode=\"overwrite\", header=True)\n",
    "    revenue_by_category.write.csv(\"output/revenue_by_category.csv\", mode=\"overwrite\", header=True)\n",
    "    print(\"Analysis results saved to the output directory.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "49f8e18d-75f4-479c-8a0b-e88ea0ff4ea7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted data/RestaurantOrders.zip to extracted_files\n",
      "Loaded DataFrame Schema:\n",
      "root\n",
      " |-- order_details_id: integer (nullable = true)\n",
      " |-- order_id: integer (nullable = true)\n",
      " |-- order_date: string (nullable = true)\n",
      " |-- order_time: string (nullable = true)\n",
      " |-- item_id: string (nullable = true)\n",
      "\n",
      "+----------------+--------+----------+-----------+-------+\n",
      "|order_details_id|order_id|order_date|order_time |item_id|\n",
      "+----------------+--------+----------+-----------+-------+\n",
      "|1               |1       |1/1/23    |11:38:36 AM|109    |\n",
      "|2               |2       |1/1/23    |11:57:40 AM|108    |\n",
      "|3               |2       |1/1/23    |11:57:40 AM|124    |\n",
      "|4               |2       |1/1/23    |11:57:40 AM|117    |\n",
      "|5               |2       |1/1/23    |11:57:40 AM|129    |\n",
      "|6               |2       |1/1/23    |11:57:40 AM|106    |\n",
      "|7               |3       |1/1/23    |12:12:28 PM|117    |\n",
      "|8               |3       |1/1/23    |12:12:28 PM|119    |\n",
      "|9               |4       |1/1/23    |12:16:31 PM|117    |\n",
      "|10              |5       |1/1/23    |12:21:30 PM|117    |\n",
      "|11              |6       |1/1/23    |12:29:36 PM|101    |\n",
      "|12              |6       |1/1/23    |12:29:36 PM|114    |\n",
      "|13              |7       |1/1/23    |12:50:37 PM|123    |\n",
      "|14              |8       |1/1/23    |12:51:37 PM|123    |\n",
      "|15              |9       |1/1/23    |12:52:01 PM|108    |\n",
      "|16              |9       |1/1/23    |12:52:01 PM|126    |\n",
      "|17              |9       |1/1/23    |12:52:01 PM|110    |\n",
      "|18              |9       |1/1/23    |12:52:01 PM|117    |\n",
      "|19              |9       |1/1/23    |12:52:01 PM|117    |\n",
      "|20              |9       |1/1/23    |12:52:01 PM|129    |\n",
      "+----------------+--------+----------+-----------+-------+\n",
      "only showing top 20 rows\n",
      "\n",
      "Orders Per Date:\n",
      "+----------+-----------+\n",
      "|order_date|order_count|\n",
      "+----------+-----------+\n",
      "|   1/20/23|        139|\n",
      "|   1/27/23|        149|\n",
      "|   1/21/23|        127|\n",
      "|    3/7/23|        139|\n",
      "|   3/12/23|        115|\n",
      "|   2/11/23|        152|\n",
      "|   2/23/23|        125|\n",
      "|   3/23/23|        131|\n",
      "|   1/31/23|        143|\n",
      "|   3/29/23|        129|\n",
      "|   1/30/23|        138|\n",
      "|    2/3/23|        153|\n",
      "|    1/2/23|        160|\n",
      "|    2/5/23|        132|\n",
      "|   1/13/23|        117|\n",
      "|    1/4/23|        106|\n",
      "|   2/21/23|        124|\n",
      "|   1/11/23|        114|\n",
      "|   3/26/23|        136|\n",
      "|   3/20/23|        148|\n",
      "+----------+-----------+\n",
      "only showing top 20 rows\n",
      "\n",
      "Item Frequency:\n",
      "+-------+----------+\n",
      "|item_id|item_count|\n",
      "+-------+----------+\n",
      "|    125|       470|\n",
      "|    124|       367|\n",
      "|    132|       420|\n",
      "|    101|       622|\n",
      "|    112|       324|\n",
      "|    113|       620|\n",
      "|    110|       360|\n",
      "|    107|       456|\n",
      "|    126|       249|\n",
      "|    131|       364|\n",
      "|    120|       489|\n",
      "|    130|       239|\n",
      "|    118|       354|\n",
      "|    104|       238|\n",
      "|    128|       207|\n",
      "|    102|       583|\n",
      "|    111|       355|\n",
      "|    103|       257|\n",
      "|    115|       123|\n",
      "|    122|       461|\n",
      "+-------+----------+\n",
      "only showing top 20 rows\n",
      "\n",
      "Peak Order Times:\n",
      "+-----------+-----------+\n",
      "| order_time|order_count|\n",
      "+-----------+-----------+\n",
      "| 7:27:47 PM|          2|\n",
      "|11:31:34 AM|          2|\n",
      "| 3:37:45 PM|          2|\n",
      "|12:33:51 PM|          1|\n",
      "|12:23:38 PM|          7|\n",
      "|12:20:31 PM|          3|\n",
      "| 3:17:51 PM|          2|\n",
      "| 5:12:51 PM|          1|\n",
      "| 1:39:29 PM|          3|\n",
      "| 4:43:26 PM|          2|\n",
      "| 6:27:56 PM|          2|\n",
      "|10:16:39 PM|          4|\n",
      "| 3:46:10 PM|          2|\n",
      "| 7:38:24 PM|          2|\n",
      "|11:18:31 AM|          1|\n",
      "|12:29:32 PM|          1|\n",
      "|12:40:41 PM|          5|\n",
      "|10:23:48 PM|          1|\n",
      "|12:39:57 PM|          4|\n",
      "| 1:28:07 PM|          1|\n",
      "+-----------+-----------+\n",
      "only showing top 20 rows\n",
      "\n",
      "Analysis results saved to the output directory.\n",
      "Unsupported file type: extracted_files/__MACOSX\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    zip_file_path = \"data/RestaurantOrders.zip\"  \n",
    "    extract_to_dir = \"extracted_files\"  \n",
    "\n",
    "    #Extract ZIP file\n",
    "    if not os.path.exists(extract_to_dir):\n",
    "        os.makedirs(extract_to_dir)\n",
    "    extract_zip(zip_file_path, extract_to_dir)\n",
    "\n",
    "    #Process each extracted file\n",
    "    for file_name in os.listdir(extract_to_dir):\n",
    "        if file_name == 'restaurant_db_data_dictionary.csv':\n",
    "            return\n",
    "        file_path = os.path.join(extract_to_dir, file_name)\n",
    "        process_restaurant_orders(file_path)\n",
    "\n",
    "    # Stop Spark Session\n",
    "    spark.stop()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bb5864b-fe78-48f1-b924-1939a4f0e9bb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
